{"cells":[{"cell_type":"code","source":["configs = {\"fs.azure.account.auth.type\": \"OAuth\",\n       \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n       \"fs.azure.account.oauth2.client.id\": \"ba12f285-fc5c-49f8-af3b-b0fccd8756e3\",\n       \"fs.azure.account.oauth2.client.secret\": dbutils.secrets.get(scope=\"databricks-secret-scope\",key=\"valuekeysecret123\"),\n       \"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/e12b7479-23cf-4748-a095-f753e218fe2e/oauth2/token\",\n       \"fs.azure.createRemoteFileSystemDuringInitialization\": \"true\"}\ndbutils.fs.mount(\nsource = \"abfss://input@datakeysiwinnerdata.dfs.core.windows.net/\",\nmount_point = \"/mnt/777\",\nextra_configs = configs)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9da56f38-6f2a-422b-b777-26816a9bf1d3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[31]: True</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[31]: True</div>"]}}],"execution_count":0},{"cell_type":"code","source":["spark.conf.set(\"fs.azure.account.key.datakeysiwinnerdata.dfs.core.windows.net\",dbutils.secrets.get(scope=\"databricks-secret-scope\",key=\"valuekeysecret123\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c53e74ad-210f-43a4-b211-02bd467fed3f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["df=spark.read.text(\"/mnt/777/flights.csv\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d27b7c53-9aaa-48f2-a257-5930c8d54218"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1285823641690583&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>df<span class=\"ansi-blue-fg\">=</span>spark<span class=\"ansi-blue-fg\">.</span>read<span class=\"ansi-blue-fg\">.</span>text<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;/mnt/777/flights.csv&#34;</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansi-cyan-fg\">text</span><span class=\"ansi-blue-fg\">(self, paths, wholetext, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    541</span>         <span class=\"ansi-green-fg\">if</span> isinstance<span class=\"ansi-blue-fg\">(</span>paths<span class=\"ansi-blue-fg\">,</span> str<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    542</span>             paths <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">[</span>paths<span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-fg\">--&gt; 543</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_df<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jreader<span class=\"ansi-blue-fg\">.</span>text<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_spark<span class=\"ansi-blue-fg\">.</span>_sc<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>PythonUtils<span class=\"ansi-blue-fg\">.</span>toSeq<span class=\"ansi-blue-fg\">(</span>paths<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    544</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    545</span>     def csv(self, path, schema=None, sep=None, encoding=None, quote=None, escape=None,\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    115</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    116</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 117</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    118</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    119</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    324</span>             value <span class=\"ansi-blue-fg\">=</span> OUTPUT_CONVERTER<span class=\"ansi-blue-fg\">[</span>type<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">(</span>answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> gateway_client<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    325</span>             <span class=\"ansi-green-fg\">if</span> answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> REFERENCE_TYPE<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 326</span><span class=\"ansi-red-fg\">                 raise Py4JJavaError(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    328</span>                     format(target_id, &#34;.&#34;, name), value)\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o1109.text.\n: Operation failed: &#34;This request is not authorized to perform this operation using this permission.&#34;, 403, HEAD, https://datakeysiwinnerdata.dfs.core.windows.net/input/flights.csv?upn=false&amp;action=getStatus&amp;timeout=90\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:237)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsClient.getPathStatus(AbfsClient.java:558)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getFileStatus(AzureBlobFileSystemStore.java:823)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.getFileStatus(AzureBlobFileSystem.java:518)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$2(DatabricksFileSystemV2.scala:764)\n\tat com.databricks.s3a.S3AExceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:66)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$1(DatabricksFileSystemV2.scala:761)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:434)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:455)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:279)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:271)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:455)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:415)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:341)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:455)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.getFileStatus(DatabricksFileSystemV2.scala:761)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:201)\n\tat org.apache.hadoop.fs.FileSystem.isDirectory(FileSystem.java:1439)\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:50)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:412)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:432)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:399)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:399)\n\tat org.apache.spark.sql.DataFrameReader.text(DataFrameReader.scala:1127)\n\tat sun.reflect.GeneratedMethodAccessor392.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n</div>","errorSummary":"Operation failed: &#34;This request is not authorized to perform this operation using this permission.&#34;, 403, HEAD, https://datakeysiwinnerdata.dfs.core.windows.net/input/flights.csv?upn=false&amp;action=getStatus&amp;timeout=90","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1285823641690583&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>df<span class=\"ansi-blue-fg\">=</span>spark<span class=\"ansi-blue-fg\">.</span>read<span class=\"ansi-blue-fg\">.</span>text<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;/mnt/777/flights.csv&#34;</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansi-cyan-fg\">text</span><span class=\"ansi-blue-fg\">(self, paths, wholetext, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    541</span>         <span class=\"ansi-green-fg\">if</span> isinstance<span class=\"ansi-blue-fg\">(</span>paths<span class=\"ansi-blue-fg\">,</span> str<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    542</span>             paths <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">[</span>paths<span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-fg\">--&gt; 543</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_df<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jreader<span class=\"ansi-blue-fg\">.</span>text<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_spark<span class=\"ansi-blue-fg\">.</span>_sc<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>PythonUtils<span class=\"ansi-blue-fg\">.</span>toSeq<span class=\"ansi-blue-fg\">(</span>paths<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    544</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    545</span>     def csv(self, path, schema=None, sep=None, encoding=None, quote=None, escape=None,\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    115</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    116</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 117</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    118</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    119</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    324</span>             value <span class=\"ansi-blue-fg\">=</span> OUTPUT_CONVERTER<span class=\"ansi-blue-fg\">[</span>type<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">(</span>answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> gateway_client<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    325</span>             <span class=\"ansi-green-fg\">if</span> answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> REFERENCE_TYPE<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 326</span><span class=\"ansi-red-fg\">                 raise Py4JJavaError(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    328</span>                     format(target_id, &#34;.&#34;, name), value)\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o1109.text.\n: Operation failed: &#34;This request is not authorized to perform this operation using this permission.&#34;, 403, HEAD, https://datakeysiwinnerdata.dfs.core.windows.net/input/flights.csv?upn=false&amp;action=getStatus&amp;timeout=90\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:237)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsClient.getPathStatus(AbfsClient.java:558)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getFileStatus(AzureBlobFileSystemStore.java:823)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.getFileStatus(AzureBlobFileSystem.java:518)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$2(DatabricksFileSystemV2.scala:764)\n\tat com.databricks.s3a.S3AExceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:66)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$1(DatabricksFileSystemV2.scala:761)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:434)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:455)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:279)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:271)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:455)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:415)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:341)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:455)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.getFileStatus(DatabricksFileSystemV2.scala:761)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:201)\n\tat org.apache.hadoop.fs.FileSystem.isDirectory(FileSystem.java:1439)\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:50)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:412)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:432)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:399)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:399)\n\tat org.apache.spark.sql.DataFrameReader.text(DataFrameReader.scala:1127)\n\tat sun.reflect.GeneratedMethodAccessor392.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["configs = {\"fs.azure.account.auth.type\": \"OAuth\",\n       \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n       \"fs.azure.account.oauth2.client.id\": \"ba12f285-fc5c-49f8-af3b-b0fccd8756e3\",\n       \"fs.azure.account.oauth2.client.secret\": dbutils.secrets.get(scope=\"databricks-secret-scope\",key=\"valuekeysecret123\"),\n       \"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/e12b7479-23cf-4748-a095-f753e218fe2e/oauth2/token\",\n       \"fs.azure.createRemoteFileSystemDuringInitialization\": \"true\"}\ndbutils.fs.mount(\nsource = \"abfss://input@datakeysiwinnerdata.dfs.core.windows.net/\",\nmount_point = \"/mnt/flightdata\",\nextra_configs = configs)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f545b781-2244-4ad6-9dca-84db274185cf"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[37]: True</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[37]: True</div>"]}}],"execution_count":0},{"cell_type":"code","source":["flightDF = spark.read.format('csv').options(header='true', inferschema='true').load(\"/mnt/flightdata/*.csv\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8b321bfa-f97d-49ba-b9f4-fc4f5c2e6a3c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-4237674995965268&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>flightDF <span class=\"ansi-blue-fg\">=</span> spark<span class=\"ansi-blue-fg\">.</span>read<span class=\"ansi-blue-fg\">.</span>format<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;csv&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>options<span class=\"ansi-blue-fg\">(</span>header<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#39;true&#39;</span><span class=\"ansi-blue-fg\">,</span> inferschema<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#39;true&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>load<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;/mnt/flightdata/*.csv&#34;</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansi-cyan-fg\">load</span><span class=\"ansi-blue-fg\">(self, path, format, schema, **options)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    202</span>         self<span class=\"ansi-blue-fg\">.</span>options<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">**</span>options<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    203</span>         <span class=\"ansi-green-fg\">if</span> isinstance<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">,</span> str<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 204</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_df<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jreader<span class=\"ansi-blue-fg\">.</span>load<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    205</span>         <span class=\"ansi-green-fg\">elif</span> path <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">not</span> <span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    206</span>             <span class=\"ansi-green-fg\">if</span> type<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">!=</span> list<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    115</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    116</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 117</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    118</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    119</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    324</span>             value <span class=\"ansi-blue-fg\">=</span> OUTPUT_CONVERTER<span class=\"ansi-blue-fg\">[</span>type<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">(</span>answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> gateway_client<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    325</span>             <span class=\"ansi-green-fg\">if</span> answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> REFERENCE_TYPE<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 326</span><span class=\"ansi-red-fg\">                 raise Py4JJavaError(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    328</span>                     format(target_id, &#34;.&#34;, name), value)\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o1283.load.\n: Operation failed: &#34;This request is not authorized to perform this operation.&#34;, 403, GET, https://datakeysiwinnerdata.dfs.core.windows.net/input?upn=false&amp;resource=filesystem&amp;maxResults=500&amp;timeout=90&amp;recursive=false, AuthorizationFailure, &#34;This request is not authorized to perform this operation. RequestId:1fc82f5a-601f-0056-706a-adcd70000000 Time:2021-09-19T15:27:11.0716365Z&#34;\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:237)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsClient.listPath(AbfsClient.java:234)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.listStatus(AzureBlobFileSystemStore.java:916)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.listStatus(AzureBlobFileSystemStore.java:878)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.listStatus(AzureBlobFileSystem.java:401)\n\tat com.databricks.backend.daemon.data.client.DBFSV2.$anonfun$listStatus$2(DatabricksFileSystemV2.scala:95)\n\tat com.databricks.s3a.S3AExceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:66)\n\tat com.databricks.backend.daemon.data.client.DBFSV2.$anonfun$listStatus$1(DatabricksFileSystemV2.scala:92)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:434)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:455)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:279)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:271)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:455)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:415)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:341)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:455)\n\tat com.databricks.backend.daemon.data.client.DBFSV2.listStatus(DatabricksFileSystemV2.scala:92)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.listStatus(DatabricksFileSystem.scala:150)\n\tat org.apache.hadoop.fs.Globber.listStatus(Globber.java:69)\n\tat org.apache.hadoop.fs.Globber.glob(Globber.java:217)\n\tat org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1657)\n\tat org.apache.spark.deploy.SparkHadoopUtil.$anonfun$globPath$1(SparkHadoopUtil.scala:261)\n\tat com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:377)\n\tat com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:363)\n\tat com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)\n\tat org.apache.spark.deploy.SparkHadoopUtil.globPath(SparkHadoopUtil.scala:263)\n\tat org.apache.spark.deploy.SparkHadoopUtil.globPathIfNecessary(SparkHadoopUtil.scala:272)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:848)\n\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)\n\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)\n\tat scala.collection.immutable.List.flatMap(List.scala:355)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:843)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:646)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:461)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:432)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:399)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:399)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:286)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n</div>","errorSummary":"Operation failed: &#34;This request is not authorized to perform this operation.&#34;, 403, GET, https://datakeysiwinnerdata.dfs.core.windows.net/input?upn=false&amp;resource=filesystem&amp;maxResults=500&amp;timeout=90&amp;recursive=false, AuthorizationFailure, &#34;This request is not authorized to perform this operation. RequestId:1fc82f5a-601f-0056-706a-adcd70000000 Time:2021-09-19T15:27:11.0716365Z&#34;","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-4237674995965268&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>flightDF <span class=\"ansi-blue-fg\">=</span> spark<span class=\"ansi-blue-fg\">.</span>read<span class=\"ansi-blue-fg\">.</span>format<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;csv&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>options<span class=\"ansi-blue-fg\">(</span>header<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#39;true&#39;</span><span class=\"ansi-blue-fg\">,</span> inferschema<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#39;true&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>load<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;/mnt/flightdata/*.csv&#34;</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansi-cyan-fg\">load</span><span class=\"ansi-blue-fg\">(self, path, format, schema, **options)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    202</span>         self<span class=\"ansi-blue-fg\">.</span>options<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">**</span>options<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    203</span>         <span class=\"ansi-green-fg\">if</span> isinstance<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">,</span> str<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 204</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_df<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jreader<span class=\"ansi-blue-fg\">.</span>load<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    205</span>         <span class=\"ansi-green-fg\">elif</span> path <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">not</span> <span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    206</span>             <span class=\"ansi-green-fg\">if</span> type<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">!=</span> list<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    115</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    116</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 117</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    118</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    119</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    324</span>             value <span class=\"ansi-blue-fg\">=</span> OUTPUT_CONVERTER<span class=\"ansi-blue-fg\">[</span>type<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">(</span>answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> gateway_client<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    325</span>             <span class=\"ansi-green-fg\">if</span> answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> REFERENCE_TYPE<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 326</span><span class=\"ansi-red-fg\">                 raise Py4JJavaError(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    328</span>                     format(target_id, &#34;.&#34;, name), value)\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o1283.load.\n: Operation failed: &#34;This request is not authorized to perform this operation.&#34;, 403, GET, https://datakeysiwinnerdata.dfs.core.windows.net/input?upn=false&amp;resource=filesystem&amp;maxResults=500&amp;timeout=90&amp;recursive=false, AuthorizationFailure, &#34;This request is not authorized to perform this operation. RequestId:1fc82f5a-601f-0056-706a-adcd70000000 Time:2021-09-19T15:27:11.0716365Z&#34;\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:237)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsClient.listPath(AbfsClient.java:234)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.listStatus(AzureBlobFileSystemStore.java:916)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.listStatus(AzureBlobFileSystemStore.java:878)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.listStatus(AzureBlobFileSystem.java:401)\n\tat com.databricks.backend.daemon.data.client.DBFSV2.$anonfun$listStatus$2(DatabricksFileSystemV2.scala:95)\n\tat com.databricks.s3a.S3AExceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:66)\n\tat com.databricks.backend.daemon.data.client.DBFSV2.$anonfun$listStatus$1(DatabricksFileSystemV2.scala:92)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:434)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:455)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:279)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:271)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:455)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:415)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:341)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:455)\n\tat com.databricks.backend.daemon.data.client.DBFSV2.listStatus(DatabricksFileSystemV2.scala:92)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.listStatus(DatabricksFileSystem.scala:150)\n\tat org.apache.hadoop.fs.Globber.listStatus(Globber.java:69)\n\tat org.apache.hadoop.fs.Globber.glob(Globber.java:217)\n\tat org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1657)\n\tat org.apache.spark.deploy.SparkHadoopUtil.$anonfun$globPath$1(SparkHadoopUtil.scala:261)\n\tat com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:377)\n\tat com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:363)\n\tat com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)\n\tat org.apache.spark.deploy.SparkHadoopUtil.globPath(SparkHadoopUtil.scala:263)\n\tat org.apache.spark.deploy.SparkHadoopUtil.globPathIfNecessary(SparkHadoopUtil.scala:272)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:848)\n\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)\n\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)\n\tat scala.collection.immutable.List.flatMap(List.scala:355)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:843)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:646)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:461)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:432)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:399)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:399)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:286)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n</div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ADFS","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1285823641690580}},"nbformat":4,"nbformat_minor":0}
