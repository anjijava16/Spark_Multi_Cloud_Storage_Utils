{"cells":[{"cell_type":"markdown","source":["## Set up Connection to Azure Event Hubs"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c82d7ed7-0714-4a90-873a-188be5379999"}}},{"cell_type":"code","source":["%scala\nimport org.apache.spark.eventhubs.{ ConnectionStringBuilder, EventHubsConf, EventPosition }\n\n// To connect to an Event Hub, EntityPath is required as part of the connection string.\n// Here, we assume that the connection string from the Azure portal does not have the EntityPath part.\nval connectionString = ConnectionStringBuilder(\"{EVENT HUB CONNECTION STRING FROM AZURE PORTAL}\")\n  .setEventHubName(\"{EVENT HUB NAME}\")\n  .build\nval eventHubsConf = EventHubsConf(connectionString)\n  .setStartingPosition(EventPosition.fromEndOfStream)\n\nvar streamingInputDF = \n  spark.readStream\n    .format(\"eventhubs\")\n    .options(eventHubsConf.toMap)\n    .load()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0ff2209f-c250-4c2c-838a-c9e903c84540"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## streamingInputDF.printSchema\n\n  root <br><pre>\n   </t>|-- body: binary (nullable = true) <br>\n   </t>|-- offset: string (nullable = false) <br>\n   </t>|-- sequenceNumber: long (nullable = false) <br>\n   </t>|-- enqueuedTime: timestamp (nullable = false) <br>\n   </t>|-- publisher: string (nullable = true) <br>\n   </t>|-- partitionKey: string (nullable = true) <br>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"df07bf9d-9fb4-4bfd-a933-0f006c1cc3c8"}}},{"cell_type":"markdown","source":["## Sample Event Payload\nThe `body` column is provided as a `binary`. After applying the `cast(\"string\")` operation, a sample could look like:\n<pre>\n{\n</t>\"city\": \"<CITY>\", \n</t>\"country\": \"United States\", \n</t>\"countryCode\": \"US\", \n</t>\"isp\": \"<ISP>\", \n</t>\"lat\": 0.00, \"lon\": 0.00, \n</t>\"query\": \"<IP>\", \n</t>\"region\": \"CA\", \n</t>\"regionName\": \"California\", \n</t>\"status\": \"success\", \n</t>\"hittime\": \"2017-02-08T17:37:55-05:00\", \n</t>\"zip\": \"38917\" \n}"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9158dadc-295f-44b2-916c-238fc5a13c4e"}}},{"cell_type":"markdown","source":["## GroupBy, Count"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ddf203ca-4f0f-47cb-880c-da1bac865448"}}},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions._\n\nvar streamingSelectDF = \n  streamingInputDF\n   .select(get_json_object(($\"body\").cast(\"string\"), \"$.zip\").alias(\"zip\"))\n    .groupBy($\"zip\") \n    .count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e7b4e005-f691-4ca4-971a-0b77ea17486f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Window"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ac7807ea-c571-4960-a23b-fb27e4eba987"}}},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions._\n\nvar streamingSelectDF = \n  streamingInputDF\n   .select(get_json_object(($\"body\").cast(\"string\"), \"$.zip\").alias(\"zip\"), get_json_object(($\"body\").cast(\"string\"), \"$.hittime\").alias(\"hittime\"))\n   .groupBy($\"zip\", window($\"hittime\".cast(\"timestamp\"), \"10 minute\", \"5 minute\", \"2 minute\"))\n   .count()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"30156209-e01c-40ae-9c99-223ca1725954"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Memory Output"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"74fc8709-4dcf-490a-a163-1e7d157454f4"}}},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.streaming.Trigger.ProcessingTime\n\nval query =\n  streamingSelectDF\n    .writeStream\n    .format(\"memory\")        \n    .queryName(\"sample\")     \n    .outputMode(\"complete\") \n    .trigger(ProcessingTime(\"25 seconds\"))\n    .start()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ae1d0958-ab38-4cb5-9dc6-90ef5c1e3ed7"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Console Output"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9f27f8bf-fc2d-4347-9609-aad6bf497b11"}}},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.streaming.Trigger.ProcessingTime\n\nval query =\n  streamingSelectDF\n    .writeStream\n    .format(\"console\")        \n    .outputMode(\"complete\") \n    .trigger(ProcessingTime(\"25 seconds\"))\n    .start()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c6356e31-7b32-4f5b-a7ea-b1b100941333"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## File Output with Partitions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e547d039-d846-4b40-935a-e975f08efbca"}}},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions._\n\nvar streamingSelectDF = \n  streamingInputDF\n   .select(get_json_object(($\"body\").cast(\"string\"), \"$.zip\").alias(\"zip\"),    get_json_object(($\"body\").cast(\"string\"), \"$.hittime\").alias(\"hittime\"), date_format(get_json_object(($\"body\").cast(\"string\"), \"$.hittime\"), \"dd.MM.yyyy\").alias(\"day\"))\n    .groupBy($\"zip\") \n    .count()\n    .as[(String, String)]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b2bd0e64-db91-4df7-98ed-31552cf81a19"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.streaming.Trigger.ProcessingTime\n\nval query =\n  streamingSelectDF\n    .writeStream\n    .format(\"parquet\")\n    .option(\"path\", \"/mnt/sample/test-data\")\n    .option(\"checkpointLocation\", \"/mnt/sample/check\")\n    .partitionBy(\"zip\", \"day\")\n    .trigger(ProcessingTime(\"25 seconds\"))\n    .start()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ed22a7a0-e048-4ac9-a226-d4c8dd48c32d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##### Create Table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"be6f50ea-5f82-4b09-9af9-9af2f2be5417"}}},{"cell_type":"code","source":["%sql CREATE EXTERNAL TABLE  test_par\n    (hittime string)\n    PARTITIONED BY (zip string, day string)\n    STORED AS PARQUET\n    LOCATION '/mnt/sample/test-data'"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"54b4da3f-7663-415f-8382-009d5ab1d201"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## JDBC Sink"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"32fb0872-f876-49f4-9ad9-2e6e117c8539"}}},{"cell_type":"code","source":["%scala\nimport java.sql._\n\nclass  JDBCSink(url:String, user:String, pwd:String) extends ForeachWriter[(String, String)] {\n      val driver = \"com.mysql.jdbc.Driver\"\n      var connection:Connection = _\n      var statement:Statement = _\n      \n    def open(partitionId: Long,version: Long): Boolean = {\n        Class.forName(driver)\n        connection = DriverManager.getConnection(url, user, pwd)\n        statement = connection.createStatement\n        true\n      }\n\n      def process(value: (String, String)): Unit = {\n        statement.executeUpdate(\"INSERT INTO zip_test \" + \n                \"VALUES (\" + value._1 + \",\" + value._2 + \")\")\n      }\n\n      def close(errorOrNull: Throwable): Unit = {\n        connection.close\n      }\n   }\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8eea9176-9ec2-48d5-a19c-bb6f4a9ecefc"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%scala\nval url=\"jdbc:mysql://<mysqlserver>:3306/test\"\nval user =\"user\"\nval pwd = \"pwd\"\n\nval writer = new JDBCSink(url,user, pwd)\nval query =\n  streamingSelectDF\n    .writeStream\n    .foreach(writer)\n    .outputMode(\"update\")\n    .trigger(ProcessingTime(\"25 seconds\"))\n    .start()\n    "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8cda647e-42a9-458a-8dcf-09a1f61c927a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## EventHubs Sink"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2456479c-432d-45c9-ac25-187e6f16d536"}}},{"cell_type":"code","source":["%scala\n// The connection string for the Event Hub you will WRTIE to. \nval connectionString = \"{EVENT HUB CONNECTION STRING}\"    \nval eventHubsConfWrite = EventHubsConf(connectionString)\n\nval query =\n  streamingSelectDF\n    .writeStream\n    .format(\"eventhubs\")\n    .outputMode(\"update\")\n    .options(eventHubsConfWrite.toMap)\n    .trigger(ProcessingTime(\"25 seconds\"))\n    .start()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9f216bfb-82b4-4a6c-b440-2d9f4a5d7e8e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## EventHubs Sink - Write test data with Rate Source"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"609a2c2f-242c-471e-9c92-8841dfd641d8"}}},{"cell_type":"code","source":["%scala\nimport org.apache.spark.eventhubs.{ ConnectionStringBuilder, EventHubsConf, EventPosition }\nimport org.apache.spark.sql.streaming.Trigger.ProcessingTime\n\n// The connection string for the Event Hub you will WRTIE to. \nval connString = \"{EVENT HUB CONNECTION STRING}\"    \nval eventHubsConfWrite = EventHubsConf(connString)\n\nval source = \n  spark.readStream\n    .format(\"rate\")\n    .option(\"rowsPerSecond\", 100)\n    .load()\n    .withColumnRenamed(\"value\", \"body\")\n    .select($\"body\" cast \"string\")\n\nval query = \n  source\n    .writeStream\n    .format(\"eventhubs\")\n    .outputMode(\"update\")\n    .options(eventHubsConfWrite.toMap)\n    .trigger(ProcessingTime(\"25 seconds\"))\n    .option(\"checkpointLocation\", \"/checkpoint/\")\n    .start()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"09b6f10e-c105-4611-9769-f2bb0b21f8d1"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Structured Streaming Event Hubs Integration","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":4094398762245377}},"nbformat":4,"nbformat_minor":0}
